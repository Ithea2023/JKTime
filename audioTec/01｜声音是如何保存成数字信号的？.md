# 01｜声音是如何保存成数字信号的？

你好，我是建元。

作为专栏的第一节课，今天我们来聊聊音频信号中的一些基础知识，带你近距离地了解一下音频这个既熟悉又陌生的领域。这节课我们重点学习一下音频信号的关键指标和常见的音频封装格式，它们算是基础中的基础，希望你能够快速掌握。

## 音频信号的关键指标

声音我们每天都会听见，似乎早已习以为常。那么我们是怎么把声音信号转换成数字信号记录下来存储和传输的呢？

声音是听觉对声波产生的感知，而声波的本质是介质的振动，比如空气的振动。那么我们只需要把这个振动信号记录下来，并用一串数字来表达振动信号振动的快慢和振动的幅度，就可以实现声音的记录。

如图1所示，以前的留声机就是通过唱片上凹槽的深浅、长短来表征声音的振幅和持续时间。

![图片](<https://static001.geekbang.org/resource/image/98/b3/985486bec1b75d7a9db5989a789019b3.png?wh=1476x768> "图1 留声机与唱片")

我们现在一般用麦克风来实现声音的采集。**那如何通过麦克风来采集声音呢？**

![](<https://static001.geekbang.org/resource/image/44/6c/441bba7d52yy4bddfb678fe91812ee6c.jpg?wh=1121x239> "图2 音频数字信号的生成")

使用麦克风的音频数字信号采集过程如图2所示：

- 首先，声波通过空气传播到麦克风的振膜。
- 然后，振膜随空气抖动的振幅大小产生相应的电学信号。**我们把这种带有声学表征的电学信号叫做模拟信号（Analog Signal）**。
- 最后，**通过A/DC（模数转换器）将模拟信号转换成数字信号（Digital Signal）**。即通过PCM（Pulse Code Modulation）脉冲编码调制对连续变化的模拟信号进行抽样、量化和编码转换成离散的数字信号。

<!-- -->

<!-- [[[read_end]]] -->

这样我们就实现了音频信号的采集，我们常说的PCM文件就是未经封装的音频原始文件或者叫做音频“裸数据”。那么具体音频的数字信号是怎么构成的呢？这就涉及到下面的3个基本概念：采样位深、采样率和通道数。

现在我们先来熟悉一下这3个概念。

### 采样位深

采样位深也就是每个采样点用多少bit来表示。比如位深是16就代表每个采样点需要16bit来进行存储。从物理意义上来说，**位深代表的是振动幅度的表达精确程度或者说粒度**。假设数字信号是一个1到-1的区间，如果位深为16bit，那么第1个bit表示正负号，并且剩下的15个bit可以表征0～32767个数，那么振幅就可以精确到1/32768的粒度了。

我们一般在网络电话中用的就是16bit的位深，这样不太会影响听感，并且存储和传输的耗费也不是很大。而在做音乐或者更高保真度要求的场景中则可以使用32bit甚至64bit的位深来减少失真。

8bit时失真就比较严重了。早期受到音频技术条件限制，很多音频都是8bit的，声音会显得比较模糊，如今也只有一些电话和对讲机等设备还有使用。但有趣的是，有的音乐就追求这种模糊感，所以“8bit”有的时候也代表一种听感朦胧的音乐艺术类型。

### 采样率

**采样率就是1秒内采集到的采样点的个数，一般用赫兹Hz来表示。**比如1秒有48000个采样点那么采样率就是48000Hz（48kHz）。

根据奈奎斯特采样定理在进行模拟/数字信号的转换过程中，当采样频率fs大于信号中最高频率fmax的2倍时（fs > 2fmax），采样之后的数字信号才可以完整地保留原始信号中的信息。也就是说采样率和保留的声音频率基本上是2倍的关系。

我们可以看看图3的频谱图来对比一下16kHz采样率和48kHz采样率的音频。

![图片](<https://static001.geekbang.org/resource/image/9e/d4/9e3cb2f9b3e4d57a02b81fbd6d3c4dd4.png?wh=1896x1162> " 图3 不同采样率（ 48kHz(上)和16kHz(下) ）的频谱能量分布")

由图3我们可以看到，16kHz采样率的音频在8kHz以上的频谱基本是没有能量的（黑色），也就是说这部分高频的信息由于采样率不够已经丢失了。从听感上来说人耳可以听到的频率范围大概是50～20kHz之间。如果采样率不够，那么和实际听感比起来声音就会显得“低沉”或者说“闷”。

**那么采样率是不是越高越好呢？**其实选用什么样的采样率是根据具体用途来决定的。

如果只是为了听见人声、听懂对方在说什么，那么为了节省传输码率我们可以把采样率降到8kHz（比如打电话）。而在网络音视频会议场景需要平衡音质和传输带宽消耗，我们一般可以使用16kHz或者32kHz的采样率。如果是开线上音乐会或者音乐直播，我们通常会用较高的采样率来保证音质，比如44.1kHz或者48kHz。更极端一点，在音乐制作录音的时候，我们会采用96kHz甚至更高的采样率来方便后续的调音和制作。

### 通道数

你可能在平时买音响的时候听过2.1声道或者5.1声道等名词，这些数字代表了有多少个播放单元。比如，2.1声道中的2指的是左右两个音箱，1指的是中间一个低音音箱（如图4所示）。每个音箱都会播放一个单独的音频，这时候就需要同时有3路音频信号同时播放，或者叫通道数为3。

![图片](<https://static001.geekbang.org/resource/image/a1/e2/a16d1e30835d37d3268b00385b9e85e2.png?wh=1212x370> "图4 2.0声道音箱(左)、立体声耳机(中)、2.1声道音箱(右)")

我们在实时在线互动的时候，由于编/解码器能力的限制（比如使用了单通道编/解码器），或者采集设备能力的限制（只能采集单通道的信号），音频信号通常为单声道的。我们听歌的时候，戴上耳机如果听到左右耳朵是不一样的，能够感觉到声音是从不同方向传过来的，那么就说明这个音频是双声道。我们通常也把这种双声道音频叫做立体声（Stereo）（如图4所示）。

除了播放需要多声道以外，采集也可能采集到多通道的数据。比如麦克风阵列采集到的原始信号，有多少个麦克风就会有多少个通道的音频信号。因此，**这里通道数的物理含义其实就是同一时间采集或播放的音频信号的总数。**

好了，这里我们已经基本掌握了音频数字信号的基本构成。现在我们来通过一个简单的计算来理解一下。假设我们有一个立体声的PCM音乐文件。它记录了1分40秒的采样率为48kHz的音频。如果这个文件的采样位深是16bit，那么这个立体声文件应该占用多大的存储空间呢？如果不经过压缩实时传输播放，又至少需要多少的带宽呢？

有了前面的知识我们就可以知道，一个PCM音频文件的存储大小就是采样位深、采样率、通道数和持续时间的累乘。即：

$${\rm\bf存储空间=采样位深\times采样率\times通道数\times时长}=16\times 48000\times 2\times 100=1.53\*10^{8}bit=18.31MB$$

而它实时传输所需的带宽就是它每秒所需的比特带宽。这可以用采样位深、采样率和通道数来得到。即：

$${\rm\bf比特带宽=采样位深\times采样率\times通道数}=16\times 48000\times 2=1.53\*10^{6}bps=1500kbps$$

## 音频的封装

我们可以看到上面例子中100秒的PCM音频文件就需要这么大的存储空间，那我们平时经常看到的音频文件格式，比如MP3、FLAC和WAV等，它们有什么区别？它们所需的存储空间一样么？

### 有损和无损音频编码封装格式

其实就算不是音频行业的人，也应该或多或少地听过有损和无损的音频编码封装格式。顾名思义，有损的音频封装格式主要是通过压缩算法把文件大小尽量减少，但是在解压缩的时候却无法完美还原音频原来的数据（即有损）。比如MP3、AAC、AMR和WMA等编码封装格式。

虽然叫做有损音频格式，但其实发展到现在，有损音频格式比如MP3一般可以达到1:10的压缩比，即存储体积为未压缩音频的十分之一。但在听感上和无损格式比起来，如果不是专业人士很难听出区别。

而无损音频封装则采用可完美还原的压缩算法，比如FLAC和APE等编码封装格式。FLAC与APE的压缩比基本相同，其中FLAC的压缩比为58.70%，而APE的压缩能力则要更高一些，压缩比为55.50%。它们都能压缩到接近源文件一半大小。无损封装甚至还可以不压缩编码，直接加个文件头作为封装，比如WAVE格式的封装。

其实简单地说，不是为了追求极致的听感，比如听音乐、音频分析，我们可以用有损压缩来节省存储体积。但是如果是拿音频信号做数据分析或者追求极致的听感还原，则必须使用无损压缩的封装格式来避免编码带来的损伤。

常见的有损和无损封装格式如下表1所示。其实很多编码封装也是支持调节压缩比来平衡音质和存储空间的，比如WMA可以同时支持无损和有损编码，而且MP3也可以调整不同的编码码率来调节音质。

![](<https://static001.geekbang.org/resource/image/df/22/df99ffdbe842eb4cd3dd4ea9c7ca8122.jpg?wh=1216x206> "表1 常见的无损和有损音频编码封装")

因为无需封装、方便存储，所以音频工程师经常把音频处理的中间变量（未封装的音频数据）存储为PCM文件。但由于PCM文件也就是裸数据文件是无法直接读取播放的。比如把PCM文件拖到音频处理软件Audacity中，会提示你先输入这个PCM文件的采样率、通道数、比特位深等信息才能进行解析播放。这样，一个不知道这些音频信息的人就无法正确地解析播放这些PCM文件了。

接下来我们主要介绍一个常用的WAVE格式的封装，这样我们在日常处理音频裸数据的时候就可以将其封装。别人在查看、播放时，就可以直接用播放软件打开了。

### WAVE文件的封装

WAVE文件作为多媒体中使用的声波文件格式之一，文件后缀名为wav。它是以RIFF格式为标准的，RIFF是英文Resource Interchange File Format的缩写。因此，每个WAVE文件的头四个字节便是“RIFF”。

**WAVE文件的封装格式十分简单。**WAVE文件由WAVE文件头部分和WAVE文件数据体部分组成，其中0～43字节存放采样率、通道数、数据部分的标识符等头信息，44字节以后的就是数据部分。简单地理解就是PCM文件加一个文件头描述文件的基本信息。具体文件头每个字节的含义可以参考表2。<br>

![](<https://static001.geekbang.org/resource/image/7b/29/7b05eda31a7b57d98776848ae9b2e929.jpg?wh=1300x1200> "表2 WAVE文件头每个字节的含义")

从表格2中我们可以看到，WAVE格式支持单、双声道的音频文件的封装，以及采样位深为8bit和16bit这两种格式。那么具体单通道和双通道数据是如何排列的呢？我们可以看一下图5。

![](<https://static001.geekbang.org/resource/image/9e/55/9e322f4bbdc02f42b891d5c957b47055.jpg?wh=1200x989> "图5 WAVE格式中数据的比特分布")

从图5我们可以看到双声道的时候，是左右声道的数据间隔排布的，这主要是为了方便可以按照时间连续成块地读取数据信息，而不是读完一个声道再读下一个。

## 小结

好了，我们今天的课程到这里就要结束了。我们先来简单回顾一下今天讲到的知识点。

首先，采样位深、采样率、通道数是描述音频信号的关键指标。采样位深越深、采样率越高，则音质越好，但同时消耗的存储和传输的资源也就越多。

其次，在编码封装时如果需要尽量缩小存储音频的体积，那么可以选用MP3等有损的编码封装格式，而需要无损、高保真的音频时，则可以采用例如FLAC等无损格式来编码封装。其中对于音频开发者而言，WAVE格式由于可以迅速快捷地封装一个音频裸数据文件而被广泛使用。

最后，在实际使用时我们就可以根据自己的使用场景来选择音频信号的关键指标和封装形式了。

记住，音频信号主要是用来给人听的，在音频处理时往往需要“眼看为虚，耳听为实”。你可以多听听不同的采样率，不同的采样位深会有什么样的听感。听着听着就会形成记忆，以后解决问题时，你甚至只需要听一耳朵就能发现问题。

## 思考题

学习了上面的知识点，现在我有一道思考题留给你。

为什么有的音频文件，文件显示的是48kHz的采样率和16bit的位深，但听起来还是很闷或者听感很差呢？

欢迎你在留言区和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。下节课再见。

